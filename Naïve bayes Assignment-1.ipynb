{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f38ddd-3ab5-40e1-8ee5-1328b8bf9b92",
   "metadata": {},
   "source": [
    "Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb62d0b6-791b-4146-9368-e806dec0bc7f",
   "metadata": {},
   "source": [
    "Bayes theorem is also known as the Bayes Rule or Bayes Law. It is used to determine the conditional probability of event A when event B has already happened. The general statement of Bayes’ theorem is “The conditional probability of an event A, given the occurrence of another event B, is equal to the product of the event of B, given A and the probability of A divided by the probability of event B.” i.e.\n",
    "\n",
    "P(A|B) = P(B|A)P(A) / P(B)\n",
    "\n",
    "\n",
    "where,\n",
    "\n",
    "P(A) and P(B) are the probabilities of events A and B\n",
    "\n",
    "P(A|B) is the probability of event A when event B happens\n",
    "\n",
    "P(B|A) is the probability of event B when A happens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395775a3-6284-4dc1-8df9-f4395be00a62",
   "metadata": {},
   "source": [
    "Q2.  What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538282b5-55c7-472d-8d25-95530884c477",
   "metadata": {},
   "source": [
    "Formula for Bayes' Theorem\n",
    "\n",
    "P(A|B) – the probability of event A occurring, given event B has occurred. P(B|A) – the probability of event B occurring, given event A has occurred. P(A) – the probability of event A. P(B) – the probability of event B.\n",
    "\n",
    "P(A|B) = P(B|A)(P(A)/P(B)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a9c5f1-1f6d-433c-9794-02323d7c8dff",
   "metadata": {},
   "source": [
    "Q3.How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d949457a-363a-4c6b-a721-a907cebf0ca2",
   "metadata": {},
   "source": [
    "Bayes' theorem is used in a wide range of practical applications across various fields. Here are some common ways it is used in practice:\n",
    "\n",
    "1. Medical Diagnosis: Bayes' theorem is employed in medical diagnosis to assess the likelihood of a patient having a particular disease given their symptoms and test results. Physicians use prior probabilities based on population data and update them with new patient-specific information to make more accurate diagnoses.\n",
    "\n",
    "2. Spam Filters: Email spam filters use Bayes' theorem to classify incoming emails as either spam or not spam. They calculate the probability that certain words or phrases in an email are associated with spam and update these probabilities based on the content of each incoming email.\n",
    "\n",
    "3. Machine Learning and Natural Language Processing: In machine learning, Bayes' theorem is used in algorithms like Naive Bayes classifiers for tasks such as text classification and sentiment analysis. It helps determine the probability of a document or text belonging to a particular category based on the words or features it contains.\n",
    "\n",
    "4. Stock Market Predictions: Some financial analysts use Bayesian models to make predictions about stock prices. They incorporate prior data and update their predictions with new market information to estimate future stock movements.\n",
    "\n",
    "5. Weather Forecasting: Meteorologists use Bayesian techniques to improve weather forecasts. They incorporate historical weather data and update their predictions with real-time data from weather sensors and satellite observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d034d30-0b81-4fac-8b05-b22a5485606f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.78%\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the Naive Bayes classifier (Gaussian Naive Bayes in this case)\n",
    "clf = GaussianNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c1eb7a-7585-481a-80df-8cba79d8ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f17543-fff5-4a8c-8589-e205cb990fa4",
   "metadata": {},
   "source": [
    "Bayes' theorem is closely related to conditional probability and provides a way to update conditional probabilities based on new evidence or information. To understand this relationship, let's first define conditional probability and then explain how Bayes' theorem relates to it.\n",
    "\n",
    "Conditional Probability:\n",
    "\n",
    "Conditional probability is the probability of an event occurring given that another event has already occurred. It is denoted as P(A|B) and can be read as \"the probability of event A occurring given that event B has occurred.\" Mathematically, it is defined as:\n",
    "\n",
    "P(A|B) = P(A and B) / P(B)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(A|B) is the conditional probability of event A given event B.\n",
    "P(A and B) is the probability of both events A and B occurring.\n",
    "P(B) is the probability of event B occurring.\n",
    "\n",
    "\n",
    "\n",
    "Bayes' Theorem:\n",
    "\n",
    "Bayes' theorem is a fundamental theorem in probability theory that provides a way to update our beliefs or probabilities about a hypothesis (an event or proposition) based on new evidence or information. It is represented as:\n",
    "\n",
    "P(A|B) = [P(B|A) * P(A)] / P(B)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(A|B) is the posterior probability of hypothesis A given evidence B.\n",
    "P(B|A) is the likelihood of evidence B given hypothesis A.\n",
    "P(A) is the prior probability of hypothesis A (before considering evidence B).\n",
    "P(B) is the probability of evidence B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15f4de1-e90b-4720-b2d0-0de64e44609f",
   "metadata": {},
   "source": [
    "Q5.How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e532b71-7f3d-47bd-aaf3-640cb0a63e2e",
   "metadata": {},
   "source": [
    "There are three common types of Naive Bayes classifiers: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. Here's how to choose the right one:\n",
    "\n",
    "1. Gaussian Naive Bayes (GNB):\n",
    "\n",
    "Data Type: Continuous or real-valued features.\n",
    "\n",
    "Assumption: Assumes that the features follow a Gaussian (normal) distribution.\n",
    "\n",
    "Example Applications:\n",
    "\n",
    "Natural language processing (NLP) when using word embeddings or continuous-valued word frequencies.\n",
    "Predicting numerical values or regression problems when dealing with continuous variables.\n",
    "\n",
    "2. Multinomial Naive Bayes (MNB):\n",
    "\n",
    "Data Type: Discrete or count-based features, often used with text data.\n",
    "Assumption: Assumes that the features follow a multinomial distribution (e.g., word frequencies in a text document).\n",
    "\n",
    "Example Applications:\n",
    "\n",
    "Text classification tasks, such as spam detection or sentiment analysis.\n",
    "\n",
    "Document classification, where you have term frequency (TF) or term frequency-inverse document frequency (TF-IDF) features.\n",
    "\n",
    "3. Bernoulli Naive Bayes (BNB):\n",
    "\n",
    "Data Type: Binary data (0/1 or True/False).\n",
    "Assumption: Assumes that features are binary-valued (0/1).\n",
    "\n",
    "Example Applications:\n",
    "\n",
    "Text classification with binary presence/absence features, such as document bag-of-words (BoW) models.\n",
    "\n",
    "Binary classification problems, like email classification (spam vs. not spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290c71e3-aa99-41ae-a157-cf24e6b21b3d",
   "metadata": {},
   "source": [
    "Q6.Assignment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3cc98a-a1af-4a89-9b3f-a995b0d7a9da",
   "metadata": {},
   "source": [
    "To predict the class of a new instance with features X1 = 3 and X2 = 4 using Naive Bayes, we can calculate the posterior probabilities for each class A and B and then choose the class with the higher posterior probability.\n",
    "\n",
    "The posterior probability of class A given the features X1 = 3 and X2 = 4 (denoted as P(A|X1=3, X2=4)) can be calculated using Bayes' theorem as follows:\n",
    "\n",
    "P(A|X1=3, X2=4) = (P(X1=3|A) * P(X2=4|A) * P(A)) / P(X1=3, X2=4)\n",
    "\n",
    "We assume equal prior probabilities for each class (P(A) = P(B) = 0.5).\n",
    "\n",
    "P(X1=3|A) is the likelihood of observing X1=3 given class A.\n",
    "P(X2=4|A) is the likelihood of observing X2=4 given class A.\n",
    "\n",
    "We can see the frequencies of feature values for class A:\n",
    "\n",
    "P(X1=3|A) = 4/10\n",
    "P(X2=4|A) = 3/10\n",
    "\n",
    "The posterior probability for class A:\n",
    "\n",
    "P(A|X1=3, X2=4) = (4/10 * 3/10 * 0.5) / P(X1=3, X2=4)\n",
    "\n",
    "To calculate P(X1=3, X2=4), we can sum up the frequencies for both classes for these feature values:\n",
    "\n",
    "P(X1=3, X2=4) = P(X1=3|A) * P(A) + P(X1=3|B) * P(B)\n",
    "= (4/10 * 0.5) + (1/10 * 0.5) = 0.2\n",
    "\n",
    "Now, we can calculate the posterior probabilities for both classes:\n",
    "\n",
    "P(A|X1=3, X2=4) = (4/10 * 3/10 * 0.5) / 0.2 = 0.3\n",
    "\n",
    " let's calculate the posterior probability for class B:\n",
    "\n",
    "P(B|X1=3, X2=4) = (P(X1=3|B) * P(X2=4|B) * P(B)) / P(X1=3, X2=4)\n",
    "= (1/10 * 3/10 * 0.5) / 0.2 = 0.075\n",
    "\n",
    "Since P(A|X1=3, X2=4) > P(B|X1=3, X2=4), the Naive Bayes classifier would predict that the new instance belongs to class A.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d5ee17-1b20-4932-a1b6-64f990f05c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d38311d-5f64-4552-91e4-fa60cd089468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3deedac-896f-4cb1-962e-eaf9186e7453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e109c8bb-1117-447d-a618-6dbf74733986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
